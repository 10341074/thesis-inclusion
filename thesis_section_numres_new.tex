\section{Numerical Results}
This section is dedicated to the discussion of the numerical approximations computed with the 
Python package \cite{}.
We consider first a smooth inclusion, elliptic shaped. Then we compare the results obtained by 
the two different methods
\begin{enumerate}
 \item the Tikhonov regularization of the Neumann--to--Dirichlet map, described by the linear
 sampling method, corresponding to equation \eqref{eq:tikh-reg-lsm},
 \item the factorization method, by the criterion described in section \ref{section:range-criterion}.
\end{enumerate}
The conductivity $k$ has been taken constant $k=2$, as its value doesn't 
affect the outcome in a relevant way. The number of quadrature nodes 
on $\partial D$ and on $\partial \Omega$ has been chosen high enough to obtain a good approximation 
of the map $N_r$.
In the sequel, we will use the abbreviations FM for the Factorization Method, LSM for the regularized 
Linear Sampling Method.
\par
We can observe in Figures 
\ref{fig:one_ellipse0}, \ref{fig:one_ellipse1}, and \ref{fig:one_ellipse2} the 
reconstruction of the inclusion through the LSM, with different values of 
the regularization parameter $\alpha$, and $m$ eigenvalues considered in the FM. We see that for small and sharp shapes, 
we need to be more accurate with smaller values for $\alpha$. Furthermore in Figure \ref{fig:one_ellipse1}
it's clear that the vicinity of the observation boundary doesn't affect the reconstruction in a relevant
way, as in other methods does.
\par
In Table \ref{tab:ellipse-parameters} we can compare the approximate inclusion 
computed through the FM with the exact shape.
The main difference between two methods is that the LSM reconstructs well the position and the shape 
of the inclusion. Moreover, the FM computes with a good accuracy also its size. 

\input{notes_thesis/fig_one_ellipse}

\noindent
Now we consider different geometries, like the kite shape (smoothly parametrized), 
and the drop shape or any polygonal inclusion, for which must be 
used a graded quadrature mesh 
(we used the parametrization \eqref{eq:graded-parametrization-power}) which excludes the 
values in the vertexes. 
The results are reported in Figures \ref{fig:first_compare_kite}, \ref{fig:first_compare_drop} 
and \ref{fig:first_compare_triangle}, which show a good approximation of the shapes and 
of the position of the different inclusions. 

\input{notes_thesis/fig_one_shape}

% factorization comments


Now we compare previous results with the approximations obtained, for the same geometries, by the 
combination of the Tikhonov regularization with the Morozov discrepancy principle, which fixes the 
error level $\delta$, and computes the norm of the regularized solutions for different values of 
$\alpha(\delta)$. For connected inclusions, we cannot observe a significant improvement. 
Instead, for a non connected domain, composed by two or three components, Figures \ref{fig:discrepancy_two_ellipse} 
and \ref{fig:discrepancy_three_ellipse}
show that after about 15 iterations on $\alpha$, contour plot fills better the unknown shape.
For further iterations, some difficulties occurs, since the root $\alpha(\delta)$ of the discrepancy 
function $\Delta_N(\alpha)$ is too close to the machine epsilon. 
We can observe that FM works still well for a high resolution, while is less accurate for a few 
number of quadrature nodes.
\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.38\textwidth]{fig/discrepancy_fm_two_ellipse0}
}
\\
\subfloat[][\emph{Morozov principle applied to LSM after $14$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_14}
}
\subfloat[][\emph{Morozov principle applied to LSM after $15$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_15}
}
\caption{Discrepancy principle applied to LSM for a domain with two components.}
\label{fig:discrepancy_two_ellipse}
\end{figure}
\end{center}

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.38\textwidth]{fig/discrepancy_fm_three_ellipse0}
}
\\
\subfloat[][\emph{Morozov principle applied to LSM after $14$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_14}
}
\subfloat[][\emph{Morozov principle applied to LSM after $15$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_15}
}
\caption{Discrepancy principle applied to LSM for a domain with three components.}
\label{fig:discrepancy_three_ellipse}
\end{figure}
\end{center}

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Plot of $\alpha$ computed by the Morozov principle after $15$ iterations}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_alpha_it_alpha_15}
}
\subfloat[][\emph{Plot of $\alpha$ computed by the Morozov principle after $15$ iterations}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_alpha_it_alpha_15}
}
\caption{Check on values of $\alpha$ computed by the Morozov discrepancy principle}
\label{fig:discrepancy_alpha}
\end{figure}
\end{center}

The \emph{Inverse Crime} effect appears every time we solve the inverse problem with data computed 
by the discretization of the direct map. It's well known in literature how this aspect leads to 
better estimates than which we should expect. Indeed, in our case, the right term $\psi_{z0}$ 
has been corrected by the direct Neumann--to--Dirichlet map (see \eqref{def:lsm-psi}). Therefore, it's advisable to 
introduce some noise in the data, such that $\|y - y^\delta\|\propto\delta$ is more realistic.

We note that the FM is more sensible to noise, while the regularized LSM is more stable, as can 
deduced by observing Figure \ref{fig:noise}. Indeed, introducing a normal distributed random noise of 
maximum amplitude equal to $\delta$, we can observe that factorization method does not work for 
values of $\delta$ greater than $\mathrm{1e}{-05}$, while the norm of the regularized solution computed with
the linear sampling method still approximates the unknown inclusion.

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$ and $\delta=\mathrm{1e}{-05}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_lsm_ellipse0_noiselevel1e-05}
}
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$ and $\delta=\mathrm{1e}{-02}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_lsm_ellipse0_noiselevel1e-02}
}
\subfloat[][\emph{Factorization method for $\delta=\mathrm{1e}{-02}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_fm_ellipse0_noiselevel1e-05}
}
\caption{Stability of the methods with respect to normal noise of amplitude $\delta$ in the data.}
\label{fig:noise}
\end{figure}
\end{center}

\clearpage
%\cleardoublepage
% \FloatBarrier

