\section{Numerical Results}
This section is dedicated to the discussion of the numerical approximations computed with the 
Python package \cite{}.
We consider first a smooth inclusion, elliptic shaped. Then we compare the results obtained by 
the two different methods
\begin{enumerate}
 \item the factorization method, by the criterion described in section \ref{section:range-criterion},
 \item the Tikhonov regularization of the Neumann--to--Dirichlet map, described by the linear
 sampling method, corresponding to equation \eqref{eq:tikh-reg-lsm}.
\end{enumerate}
The conductivity $k$ has been taken constant $k=2$, as its value doesn't 
affect the outcome in a relevant way. The number of quadrature nodes 
on $\partial D$ and on $\partial \Omega$ has been chosen high enough to obtain a good approximation 
of the map $N_r$.
In the sequel, we will use the abbreviations FM for the Factorization Method, LSM for the regularized 
Linear Sampling Method.
\par
As can be observed in Figures \ref{fig:one_ellipse0} and \ref{fig:one_ellipse1} and 
in Table \ref{tab:ellipse-parameters}, the LSM reconstructs well the position and the shape 
of the inclusion. Moreover, the FM computes with a good accuracy also its size. 

\begin{table}
\caption{\emph{Elliptic inclusion's parameters computed}.}
\label{tab:ellipse-parameters}
\begin{center}
\vspace*{-0.5cm}
\begin{tabular}{ccccc}
\toprule
 & exact && exact &\\
 fig & center & center & axes  & axes \\
\midrule
\ref{fig:one_ellipse0} & (0, 0) &  (0.0046, -0.0041) & (2, 1) & (1.9224, 0.7561) \\
\ref{fig:one_ellipse1} & (0, 0) &  (1.0797, 0.0253) & (1.8, 1.2) & (1.8340, 1.0232) \\
\bottomrule
\end{tabular}
\end{center}
% \vspace*{-1.5cm}
\end{table}

\iftoggle{fig}{
\begin{center}
\begin{figure}
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$}.]
{
\includegraphics[width=.48\textwidth]{fig/one_ellipse_lsm_ellipse0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.48\textwidth]{fig/one_ellipse_fm_ellipse0}
}
\caption{Shape and size approximations for an elliptic inclusion.}
\label{fig:one_ellipse0}
\end{figure}
\end{center}
}

\iftoggle{fig}{
\begin{center}
\begin{figure}
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$}.]
{
\includegraphics[width=.48\textwidth]{fig/one_ellipse_lsm_ellipse1}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.48\textwidth]{fig/one_ellipse_fm_ellipse1}
}
\caption{Shape and size approximations for an elliptic inclusion, not centered in $\Omega$.}
\label{fig:one_ellipse1}
\end{figure}
\end{center}
}

Now we consider different geometries, like the most used in literature: the kite shape (smoothly parametrized), 
and the drop shape or any polygonal inclusion, for which must be used a graded quadrature mesh 
(we used the parametrization \eqref{eq:graded-parametrization-power}) which excludes the values in the vertexes. 
The results are reported in Figures \ref{fig:first_compare_kite}, \ref{fig:first_compare_drop} 
and \ref{fig:first_compare_triangle}, which show a good approximation of the shapes and 
of the position of the different inclusions. 
\begin{center}
\begin{figure}[]%[tb]
\subfloat[][\emph{Linear sampling method for for $\alpha=\mathrm{1e}{-10}$}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_lsm_kite0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_fm_kite0}
}
\caption{Comparison between two method for a kite shaped inclusion.}
\label{fig:first_compare_kite}
\end{figure}
\end{center}
\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_lsm_drop0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_fm_drop0}
}
\caption{Comparison between two method for a drop shaped inclusion.}
\label{fig:first_compare_drop}
\end{figure}
\end{center}
\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_lsm_triangle0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.48\textwidth]{fig/first_compare_fm_triangle0}
}
\caption{Comparison between two method for a triangle shaped inclusion.}
\label{fig:first_compare_triangle}
\end{figure}
\end{center}
Now we compare previous results with the approximations obtained, for the same geometries, by the 
combination of the Tikhonov regularization with the Morozov discrepancy principle, which fixes the 
error level $\delta$, and computes the norm of the regularized solutions for different values of 
$\alpha(\delta)$. For connected inclusions, we cannot observe a significant improvement. 
Instead, for a non connected domain, composed by two or three components, Figures \ref{fig:discrepancy_two_ellipse} 
and \ref{fig:discrepancy_three_ellipse}
show that after about 15 iterations on $\alpha$, contour plot fills better the unknown shape.
For further iterations, some difficulties occurs, since the root $\alpha(\delta)$ of the discrepancy 
function $\Delta_N(\alpha)$ is too close to the machine epsilon. 
We can observe that FM works still well for a high resolution, while is less accurate for a few 
number of quadrature nodes.
\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.38\textwidth]{fig/discrepancy_fm_two_ellipse0}
}
\\
\subfloat[][\emph{Morozov principle applied to LSM after $14$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_14}
}
\subfloat[][\emph{Morozov principle applied to LSM after $15$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_it_alpha_15}
}
\caption{Discrepancy principle applied to LSM for a domain with two components.}
\label{fig:discrepancy_two_ellipse}
\end{figure}
\end{center}

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_0}
}
\subfloat[][\emph{Factorization method}.]
{
\includegraphics[width=.38\textwidth]{fig/discrepancy_fm_three_ellipse0}
}
\\
\subfloat[][\emph{Morozov principle applied to LSM after $14$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_14}
}
\subfloat[][\emph{Morozov principle applied to LSM after $15$ iterations in $\alpha$}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_it_alpha_15}
}
\caption{Discrepancy principle applied to LSM for a domain with three components.}
\label{fig:discrepancy_three_ellipse}
\end{figure}
\end{center}

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Plot of $\alpha$ computed by the Morozov principle after $15$ iterations}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_two_ellipse_delta1e-60_alpha_it_alpha_15}
}
\subfloat[][\emph{Plot of $\alpha$ computed by the Morozov principle after $15$ iterations}.]
{
\includegraphics[width=.48\textwidth]{fig/discrepancy_lsm_three_ellipse_delta1e-60_alpha_it_alpha_15}
}
\caption{Check on values of $\alpha$ computed by the Morozov discrepancy principle}
\label{fig:discrepancy_alpha}
\end{figure}
\end{center}

The \emph{Inverse Crime} effect appears every time we solve the inverse problem with data computed 
by the discretization of the direct map. It's well known in literature how this aspect leads to 
better estimates than which we should expect. Indeed, in our case, the right term $\psi_{z0}$ 
has been corrected by the direct Neumann--to--Dirichlet map (see \eqref{def:lsm-psi}). Therefore, it's advisable to 
introduce some noise in the data, such that $\|y - y^\delta\|\propto\delta$ is more realistic.

We note that the FM is more sensible to noise, while the regularized LSM is more stable, as can 
deduced by observing Figure \ref{fig:noise}. Indeed, introducing a normal distributed random noise of 
maximum amplitude equal to $\delta$, we can observe that factorization method does not work for 
values of $\delta$ greater than $\mathrm{1e}{-05}$, while the norm of the regularized solution computed with
the linear sampling method still approximates the unknown inclusion.

\begin{center}
\begin{figure}%[tb]
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$ and $\delta=\mathrm{1e}{-05}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_lsm_ellipse0_noiselevel1e-05}
}
\subfloat[][\emph{Linear sampling method for $\alpha=\mathrm{1e}{-10}$ and $\delta=\mathrm{1e}{-02}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_lsm_ellipse0_noiselevel1e-02}
}
\subfloat[][\emph{Factorization method for $\delta=\mathrm{1e}{-02}$}.]
{
\includegraphics[width=.30\textwidth]{fig/noise_fm_ellipse0_noiselevel1e-05}
}
\caption{Stability of the methods with respect to normal noise of amplitude $\delta$ in the data.}
\label{fig:noise}
\end{figure}
\end{center}

\clearpage
%\cleardoublepage
% \FloatBarrier

